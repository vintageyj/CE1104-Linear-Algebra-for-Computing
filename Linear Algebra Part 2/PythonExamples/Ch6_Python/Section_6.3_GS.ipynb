{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GramSchmidt\n",
    "# Author: Zhang Su (Teaching Assistant)\n",
    "# Using python3, numpy\n",
    "# 3 June 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Outcome\n",
    "\n",
    "**The code in this demo largely beyonds the scope of the lecture. But it provides lots of facts that might interest you.** \n",
    "\n",
    "**Though this demo is mostly for your interest, the Practice is helpful and important for furthering your understanding.** \n",
    "\n",
    "By the end of this material, you should be able to:\n",
    "\n",
    "+ Describe the difference between the theoretical and practical GS process and the way to alleviate the trouble caused by rounding errors.\n",
    "+ Witness the comparison of the QR factorization using the classical and modified GS process.\n",
    "\n",
    "Note: \n",
    "1. If you occasionally double clicked a textual cell, the display would change to markdown source code. To reverse, simply click anywhere of that markdown cell,  and then click **Run** in the top manu.\n",
    "2. Sometimes the notebook may not be responding. That is caused by the failure of jupyter kernel. To repair, try clicking **Kernel** in the top manu, then clicking **Reconnect**. \n",
    "3. Section Takeaways summarizes useful tips, e.g., holes of Python to avoid, if any.\n",
    "4. Section Practice reflect the learning outcomes. You are expected to solve them based on your understanding on the lecture notes alone with the coding skills learned from this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents <a name=\"Table_of_Content\"></a>\n",
    "+ 6.3.2. [Gram_Schmidt](#Gram_Schmidt) \n",
    "+ 6.3.3. [QR Factorization by Gram Schmidt](#QR_Factorization)\n",
    "+ [Classical Gram-Schmidt VS. Modified Gram-Schmidt](#vs) (For your interests)\n",
    "+ [Takeaways](#Takeaways)\n",
    "+ [Practice](#Practice)\n",
    "\n",
    "\n",
    "Let's import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gram-Schmidt Process<a name=\"Orthonormal_Sets\"></a>\n",
    "[Return to Table of Content](#Table_of_Content)\n",
    "\n",
    "**This part of code is for Section 6.3.2.**\n",
    "\n",
    "![title](img/gram_schmidt.png)\n",
    "> Figure 1. The Gram-Schmidt process. Note that the process shown in the figure does not include the normalization. Though $Q=\\{v_1,v_2,\\ldots,v_p\\}$ can already form an orthogonal set, it would be better if they are further normalized to be an orthonormal set. Tutorial 6.2 provides plenty of explanations on this.\n",
    "\n",
    "Let's implement the GS process following the notation of Fig. 1. Given a $m\\times n$ matrix $A=\\{x_1, x_2, \\ldots, x_p\\}$, the goal is to obtain a new $m\\times n$ matrix $Q=\\{v_1,v_2,\\ldots,v_p\\}$, whose columns are mutually orthonormal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_schmidt(A):\n",
    "# The classical Gram Schmidt process.\n",
    "# Input: an arbitrary mxp matrix.\n",
    "# Output: the Q factors of the input.   \n",
    "\n",
    "    n = A.shape[1]\n",
    "    Q = np.zeros(A.shape)\n",
    "    \n",
    "    for i in range(n):\n",
    "        xi = A[:, i]\n",
    "        vi = xi\n",
    "        for j in range(i):\n",
    "            # Since Q[:,j] is always a unit vector,\n",
    "            # no need to implement the denominator. \n",
    "            vi = vi - np.dot(xi, Q[:,j])*Q[:,j] \n",
    "\n",
    "        Q[:, i] = vi/la.norm(vi)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder why the denominator is missing in Line 10. The reason is that in the first iteration, i.e., when `i=0`, the nested iteration in Line 9 will not trigger. Therefore, the only thing achieved in the first iteration is the normalization of $x_1$, which is saved as $v_1$. In the second (and further iterations), `Q[:, j]` in Line 10 equals the normalized $v_1$. Since $v_1\\cdot v_1=1$, the denominator can be omitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gram-Schmidt Process yields \n",
      " [[ 0.9486833  -0.16903085]\n",
      " [ 0.          0.84515425]\n",
      " [-0.31622777 -0.50709255]]\n",
      "Is Q orthonormal? True\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[3,8],\n",
    "            [0,5],\n",
    "            [-1,-6]], dtype=float)\n",
    "\n",
    "Q = gram_schmidt(A)\n",
    "\n",
    "print(\"The Gram-Schmidt Process yields \\n\", Q)\n",
    "print(\"Is Q orthonormal?\", np.allclose(Q.T.dot(Q), np.eye(Q.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR Factorization by Gram Schmidt<a name=\"QR_Factorization\"></a>\n",
    "[Return to Table of Content](#Table_of_Content)\n",
    "\n",
    "\n",
    "Now we come to the QR factorization (using Gram-Schmidt process)! Actually, there are many ways to implement the Gram-Schmidt process as shown in Fig. 1. Here are two *very different* implementations:\n",
    "\n",
    "![title](img/compare_gram.png)\n",
    ">Figure 2. The comparison of two typical implementations.\n",
    "\n",
    "They are arithmetically equivalent, i.e., they do the same task as shown in Fig. 1. In exact arithmetic the two implementations are identical, while in the presence of rounding errors they are dramatically different.\n",
    "\n",
    "In classical Gram-Schmidt, we compute the length of the orthogonal projections of $w=a_k$ onto $q_1, q_2, \\ldots, q_{k-1}$, and then subtract those projections (with the rounding errors) from $w$. But because of rounding errors, $q_1, q_2, \\ldots, q_{k-1}$ are not truly orthogonal. It accumulates errors.\n",
    "\n",
    "In modified Gram-Schmidt, we compute the length of the projection of $w=a_k$ onto $q_1$ and subtract that projection (and the rounding errors) from $w$. Next we compute the length of the projection of the **computed** $w$ onto $q_2$ and subtract that projection (and the rounding errors) from $w$, and so on, but always orthogonalizing against the **computed version of $w$**.\n",
    "\n",
    "The difference only emerges when $k>=3$.\n",
    "\n",
    "Let us implement them first and run some comparisons. Note that in Fig. 1, the output is the Q factor of QR factorization. In our implementations we would also output the R factor following Fig. 2.\n",
    "\n",
    "Be ware of the pythonic [**memory issue**](https://ipython-books.github.io/45-understanding-the-internals-of-numpy-to-avoid-unnecessary-array-copying/), the operations of any `Q[i, j] = z` affect the memory block. An example is provided in **Section Takeaways**.\n",
    "\n",
    "Also, the `qr_gram_schmidt()` implemented below is an extension of `gram_schmidt()` in the first part. Compared to `gram_schmidt()`, `qr_gram_schmidt()` is written in a more economical manner, with the factor R outputted. \n",
    "\n",
    "**Challenge: can you revise `gram_schmidt()` following the idea of modified Gram-Schmidt discribed above?** You are expected to revise the routine `gram_schmidt()` provided in the first part of this demo. No need to output the R factor. (Don't worry, this is only for your interest :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_gram_schmidt(A):\n",
    "    # The QR factorization using the classical Gram Schmidt process.\n",
    "    # Input: an arbitrary mxp matrix.\n",
    "    # Output: the Q and R factors of the input.\n",
    "\n",
    "    # Obtain the column number.\n",
    "    p = A.shape[1]\n",
    "\n",
    "    # Initiate the Q and R factors.\n",
    "    Q = A.copy() # Prevent A from being changed.\n",
    "    R = np.zeros((p, p))\n",
    "\n",
    "    # Iterate for each column of the input.\n",
    "    for k in range(p):\n",
    "        w = A[:, k]\n",
    "        \n",
    "        ##### Different part. #####\n",
    "        for i in range(k):\n",
    "            R[i, k] = Q[:, i].dot(w)\n",
    "\n",
    "        for i in range(k):\n",
    "            w = w - R[i, k] * Q[:, i]\n",
    "        ###########################\n",
    "        \n",
    "        R[k, k] = la.norm(w)\n",
    "        Q[:, k] = w / R[k, k]\n",
    "\n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_mod_gram_schmidt(A):\n",
    "# The QR factorization using the modified Gram Schmidt process.\n",
    "# Input: an arbitrary mxp matrix.\n",
    "# Output: the Q and R factors of the input.\n",
    "\n",
    "    # Obtain the column number.\n",
    "    p = A.shape[1]\n",
    "    \n",
    "    # Initiate the Q and R factors.\n",
    "    Q = A.copy() # Prevent A from being changed.\n",
    "    R = np.zeros((p,p))\n",
    "    \n",
    "    # Iterate for each column of the input.\n",
    "    for k in range(p):\n",
    "        w = A[:, k]\n",
    "        \n",
    "        ##### Different part. #####\n",
    "        for i in range(k):\n",
    "            R[i, k] = Q[:, i].dot(w)\n",
    "            w = w - R[i, k] * Q[:, i]\n",
    "        ###########################\n",
    "        \n",
    "        R[k, k] = la.norm(w)\n",
    "        Q[:, k] = w / R[k, k]\n",
    "            \n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Gram-Schmidt VS. Modified Gram-Schmidt<a name=\"vs\"></a>\n",
    "[Return to Table of Content](#Table_of_Content)\n",
    "\n",
    "A simple experiment is carried out to compare the performance of these two algorithms.\n",
    "\n",
    "#### Critera\n",
    "\n",
    "We employ the reconstruction error and orthogonal error to measure the performance of the QR factorization implementations. The motivations are explained as follows.\n",
    "\n",
    "Given a square matrix $A$, we carry out QR factorization and obtain $A=QR$. There are two properties we perticularly expect. \n",
    "\n",
    "1. The two factors $Q$ and $R$ can reconstruct $A$ precisely.\n",
    "2. The $Q$ factor holds the orthonormality.\n",
    "\n",
    "For property No. 1, we can examine the matrix $A-QR$. Ideally, $A-QR$ should yield a matrix with all zeros. For property No. 2, we can examine the matrix $I-Q^TQ$. Ideally, $I-Q^TQ$ should also be a zero matrix.\n",
    "\n",
    "Yet, reality is always so crucial, no? We will soon see that the matrices $A-QR$ and $I-Q^TQ$ are not valued as expected. To measure to what extent can the actual $Q$ and $R$ hold the properities, we employ **infinity norm**. So what is it? Let's see two examples.\n",
    "\n",
    "Given a vector $u=(1,2,-3,4,-5)$, its infinity norm $\\|u\\|_\\infty=5$. Here, $5$ is the absolute value of the fifth entry. Of all the entries, $-5$ has the largest absolute value. In sum, the infinity norm for a vector returns the absolute of the entry with the largest absolute value.\n",
    "\n",
    "Given a matrix $B=\\begin{pmatrix}\n",
    "1& -7\\\\\n",
    "-2 & -3\n",
    "\\end{pmatrix}$, its infinity norm $\\|A\\|_\\infty=8$. Here, $8$ is the sum of the absolute values of the first row. Of all the rows, the first row has the largest absolute value, i.e., $|1|+|-7|=1+7=8>|-2|+|-3|=5$. In sum, the infinity norm for a matrix returns the sum of absolute values of the row with the largest sum of absolute values over its entries.\n",
    "\n",
    "\n",
    "In this demo, we employ the infinity norm of matrix, because we are interested to see the most deviant row in matrix $A-QR$ and $I-Q^TQ$. Since the entries of these two matrix might be mostly approaching 0, using our farmiliar 2-norm may not be able to reveal the difference.\n",
    "\n",
    "Note, I deliberately avoid of formal mathematical notations. You may refer to [this link](https://www.netlib.org/lapack/lug/node75.html) for more solid definitions or examples of all norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(Q, R, A):\n",
    "# Input: the Q, and R factors of A, and the A matrix.\n",
    "# Output: reconstruction_error is the infinity norm of matrix (QR - A).\n",
    "#         orthogonal_error is the infinity norm of matrix (QtQ-I)\n",
    "# Given a matrix, the infinity norm returns the sum of \n",
    "# absolute values of the row with the largest sum of absolute \n",
    "# values over its entries.\n",
    "\n",
    "    reconstruction_error = la.norm(Q.dot(R) - A, ord=np.inf) / la.norm(A, ord=np.inf)\n",
    "    orthogonal_error = la.norm(Q.T.dot(Q)-np.eye(R.shape[0]), ord=np.inf)\n",
    "    return reconstruction_error, orthogonal_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On a matrix with two columns\n",
    "\n",
    "First let's try the `3x2` matrix from Q7 of Tutorial 6.3. According to the pseudo-code of Fig. 2, there should be no differece (Difference emerges only when column number is greater than 2). Note that you have to **specify the data type** as `dtype=float` for the matrix. Otherwise, the matrix `A` will be considered as an integer data (since we manyally assigned integers), which may produce wrong output from dividing. We shall obtain an orthonormal version of Q compared to the solution of this tutorial question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The should-be-orthonormal matrix Q =\n",
      " [[ 0.9487 -0.169 ]\n",
      " [ 0.      0.8452]\n",
      " [-0.3162 -0.5071]]\n",
      "The reconstruction error =\n",
      " 0.0\n",
      "The orthogonal error =\n",
      " 1.3877787807814457e-16\n",
      "The should-be-orthonormal matrix Q =\n",
      " [[ 0.9487 -0.169 ]\n",
      " [ 0.      0.8452]\n",
      " [-0.3162 -0.5071]]\n",
      "The reconstruction error =\n",
      " 0.0\n",
      "The orthogonal error =\n",
      " 1.3877787807814457e-16\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[3,8],\n",
    "            [0,5],\n",
    "            [-1,-6]], dtype=float)\n",
    "\n",
    "Q, R = qr_gram_schmidt(A)\n",
    "\n",
    "reconstruction_error, orthogonal_error = compare(Q, R, A)\n",
    "print('The should-be-orthonormal matrix Q =\\n', Q.round(4))\n",
    "print('The reconstruction error =\\n', reconstruction_error)\n",
    "print('The orthogonal error =\\n', orthogonal_error)\n",
    "\n",
    "Q, R = qr_mod_gram_schmidt(A)\n",
    "\n",
    "reconstruction_error, orthogonal_error = compare(Q, R, A)\n",
    "print('The should-be-orthonormal matrix Q =\\n', Q.round(4))\n",
    "print('The reconstruction error =\\n', reconstruction_error)\n",
    "print('The orthogonal error =\\n', orthogonal_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On a matrix with three columns\n",
    "\n",
    "Next let's try the 3x3 matrix from one of our tutorial questions. Since `A` is very well specified, both two methods can obtain perfect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The should-be-orthonormal matrix Q =\n",
      " [[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "The reconstruction error =\n",
      " 0.0\n",
      "The orthogonal error =\n",
      " 0.0\n",
      "The should-be-orthonormal matrix Q =\n",
      " [[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "The reconstruction error =\n",
      " 0.0\n",
      "The orthogonal error =\n",
      " 0.0\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2, 4],\n",
    "            [0,0,5],\n",
    "            [0,3,6]], dtype=float)\n",
    "\n",
    "Q, R = qr_gram_schmidt(A)\n",
    "\n",
    "reconstruction_error, orthogonal_error = compare(Q, R, A)\n",
    "print('The should-be-orthonormal matrix Q =\\n', Q.round(4))\n",
    "print('The reconstruction error =\\n', reconstruction_error)\n",
    "print('The orthogonal error =\\n', orthogonal_error)\n",
    "\n",
    "Q, R = qr_mod_gram_schmidt(A)\n",
    "\n",
    "reconstruction_error, orthogonal_error = compare(Q, R, A)\n",
    "print('The should-be-orthonormal matrix Q =\\n', Q.round(4))\n",
    "print('The reconstruction error =\\n', reconstruction_error)\n",
    "print('The orthogonal error =\\n', orthogonal_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On a 50-by-50 Helbert matrix \n",
    "\n",
    "Let's try something \"harder\", a $50\\times 50$ [Hilbert matrix](https://en.wikipedia.org/wiki/Hilbert_matrix)! We can see that since the matrix is [not well conditioned](https://en.wikipedia.org/wiki/Condition_number), the loss on orthogonality are both obvious, where the result from the modified one is relatively good. (We can also have a sense of how terrible the accumulated rounding errors can be!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reconstruction error =\n",
      " 1.010172945062073e-16\n",
      "The orthogonal error =\n",
      " 42.85021637056607\n",
      "The reconstruction error =\n",
      " 9.63905481929459e-17\n",
      "The orthogonal error =\n",
      " 4.666330075235493\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import hilbert as hilb\n",
    "\n",
    "n = 50\n",
    "A = hilb(n)\n",
    "\n",
    "Q, R = qr_gram_schmidt(A)\n",
    "reconstruction_error, orthogonal_error = compare(Q, R, A)\n",
    "print('The reconstruction error =\\n', reconstruction_error)\n",
    "print('The orthogonal error =\\n', orthogonal_error)\n",
    "\n",
    "Q, R = qr_mod_gram_schmidt(A)\n",
    "reconstruction_error, orthogonal_error = compare(Q, R, A)\n",
    "print('The reconstruction error =\\n', reconstruction_error)\n",
    "print('The orthogonal error =\\n', orthogonal_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On a very bad conditioned matrix\n",
    "\n",
    "Finally, lets try a very special full-rank matrix:\n",
    "\n",
    "$$\n",
    "A=\\left[\\begin{array}{rrr}\n",
    "1 & 1 & 1 \\\\\n",
    "0.00000001 & 0.00000001 & 0 \\\\\n",
    "0.00000001 & 0 & 0.00000001\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "We will see that the *should-be-orthonormal* matrix $Q$ produced by the classical Gram-Schmidt is not orthonormal at all! \n",
    "\n",
    "As shown in the results, **the second and third columns of Matrix $Q$** from the classicial Gram-Schmidt have a $45^\\circ$ angle, which is a terrible failure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reconstruction error =\n",
      " 0.0\n",
      "The orthogonal error =\n",
      " 0.7071067953286833\n",
      "The should-be-orthonormal matrix Q =\n",
      " [[ 1.      0.      0.    ]\n",
      " [ 0.      0.     -0.7071]\n",
      " [ 0.     -1.     -0.7071]]\n",
      "The second and third columns above have an angle of 45 degree.\n",
      "\n",
      "The reconstruction error =\n",
      " 0.0\n",
      "The orthogonal error =\n",
      " 2e-08\n",
      "The should-be-orthonormal matrix Q =\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0. -1.]\n",
      " [ 0. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-8\n",
    "\n",
    "A = np.array([\n",
    "    [1,  1,  1],\n",
    "    [eps,eps,0],\n",
    "    [eps,0,  eps]\n",
    "    ])\n",
    "\n",
    "Q, R = qr_gram_schmidt(A)\n",
    "reconstruction_error, orthogonal_error = compare(Q, R, A)\n",
    "\n",
    "print('The reconstruction error =\\n', reconstruction_error)\n",
    "print('The orthogonal error =\\n', orthogonal_error)\n",
    "print('The should-be-orthonormal matrix Q =\\n', Q.round(4))\n",
    "print('The second and third columns above have an angle of 45 degree.\\n')\n",
    "\n",
    "Q, R = qr_mod_gram_schmidt(A)\n",
    "reconstruction_error, orthogonal_error = compare(Q, R, A)\n",
    "\n",
    "print('The reconstruction error =\\n', reconstruction_error)\n",
    "print('The orthogonal error =\\n', orthogonal_error)\n",
    "print('The should-be-orthonormal matrix Q =\\n', Q.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there exists other implementations for the Gram_Schmidt, e.g., the [householder method](https://atozmath.com/example/MatrixEv.aspx?he=e&q=qrdecomphh). You may dig more if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways<a name=\"Takeaways\"></a>\n",
    "[Return to Table of Content](#Table_of_Content)\n",
    "\n",
    "1. The classical and modified Gram-Schmidt are arithmetically equivalent, they are different when we are running them on the computer due to rounding errors.\n",
    "2. Pay attention to the difference of how Matlab and Python [handle the memory](https://ipython-books.github.io/45-understanding-the-internals-of-numpy-to-avoid-unnecessary-array-copying/), as shown in the next cell. That's why the `A.copy()` is used in the implementation of GS process (Line 10 of `gram_schmidt()` and `mod_gram_schmidt()`). The `.copy()` method prevent the alternation of the original `A`, by which `A` will not be changed after calling the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original A1 =  [[1 1]]\n",
      "The original A2 =  1\n",
      "The actual A1 after calling the function =  [[0 1]]\n",
      "The actual A2 after calling the function =  1\n"
     ]
    }
   ],
   "source": [
    "def memory_handle_test(B1, B2):    \n",
    "    B1[0,0] = 0  # Assign 0 to the first entry of array B1\n",
    "    B2 = 0 # Assign 0 to the scalar B2\n",
    "    C = 2\n",
    "    return C\n",
    "\n",
    "##############################################\n",
    "\n",
    "\n",
    "A1 = np.array([[1,1]])\n",
    "A2 = 1\n",
    "\n",
    "print('The original A1 = ', A1)\n",
    "print('The original A2 = ', A2)\n",
    "\n",
    "B = memory_handle_test(A1, A2)\n",
    "\n",
    "print('The actual A1 after calling the function = ', A1)\n",
    "print('The actual A2 after calling the function = ', A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MATLAB, `A1` and `A2` will not be changed after calling the function. However, in Python, `A1` is changed, because `B1[0,0] = 0` changed the value of the memory block. This may confuse you if you are new to Python from MATLAB, like me :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice<a name=\"Practice\"></a>\n",
    "[Return to Table of Content](#Table_of_Content)\n",
    "\n",
    "**This part can get you ready for the lab!**\n",
    "\n",
    "Given $A=\\begin{pmatrix}\n",
    "1 & 2 & 4 \\\\\n",
    "0 & 0 & 5 \\\\\n",
    "0 & 3 & 6 \n",
    "\\end{pmatrix}$ and\n",
    "\n",
    "Given $B=\\begin{pmatrix}\n",
    "1 & 2 & 4 \\\\\n",
    "1 & 2 & 5 \\\\\n",
    "1 & 2 & 6 \n",
    "\\end{pmatrix}$.\n",
    "\n",
    "1. Manually find the rank of $A$ and $B$. using pens with row reduction. Compare your result against `la.matrix_rank()` from `Numpy`.\n",
    "2. Apply the `qr_gram_schmidt()` on $A$. Print the $Q$ and $R$ factors.\n",
    "3. Verify that if $Q$ is an orthogonal matrix as we did in Section 6.2.4 of the last demo.\n",
    "3. Report the rank of the augmented matrix $[A\\,\\,Q]$ using `Numpy`. What it implies?\n",
    "4. Apply the `qr_gram_schmidt()` on $B$. Print the $Q$ and $R$ factors.\n",
    "5. Verify that if $Q$ is an orthogonal matrix as we did in Section 6.2.4 of the last demo.\n",
    "6. Why it failed on this $B$?\n",
    "7. Describe how to *adjust* B to achieve a better result? (Column swapping is allowed only)\n",
    "8. `numpy.linalg.qr()` is the routine from Numpy for QR factorization. It utilizes other algorithms for orthogonalization so that it can overcome the issue caused by linear dependency. Apply it on the $B$, print the $Q$ and $R$ factors, and verify the orthonormality of $Q$.\n",
    "9. Report the rank of the augmented matrix $[B\\,\\,Q]$ using `Numpy` again. What it implies compared to that from question 4?\n",
    "10. Based on whether the given matrix is linearly independent on columns, conclude the functionality of the QR factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
